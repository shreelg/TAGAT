{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_gc4x_BJ8f2"
      },
      "outputs": [],
      "source": [
        "!pip install torch pandas numpy matplotlib scikit-learn seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `TAGAT (modular transformer)`"
      ],
      "metadata": {
        "id": "jybNrNcIVZMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "#config class\n",
        "class Config:\n",
        "    vocab_size = 30522\n",
        "    d_model = 128\n",
        "    num_heads = 4\n",
        "    num_experts = 4\n",
        "    top_k = 2\n",
        "    num_layers = 1\n",
        "    max_seq_len = 64\n",
        "    batch_size = 32\n",
        "    num_classes = 3  # 3 classes for MultiNLI\n",
        "    lr = 1e-4\n",
        "    lambda_entropy = 1 # 1/1 for specialized, 2/3 for balanced\n",
        "    lambda_balance = 1\n",
        "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Config()\n",
        "\n",
        "\n",
        "#positional encoding\n",
        "class Positional_encoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len = 512):\n",
        "    super().__init__()\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term) #even indices\n",
        "    pe[:, 1::2] = torch.cos(position *div_term) #odd indices\n",
        "\n",
        "    self.pe = pe.unsqueeze(0) #add extra dimension (easier to broadcast when adding pos eoncoding to input tensor x)\n",
        "\n",
        "    def forward(self, x):\n",
        "      return x + self.pe[:, :x.size(1)].to(x.device) #add input tensor x + pos enc\n",
        "\n",
        "#expert submodules\n",
        "class Expert(nn.module):\n",
        "  super().__init__()\n",
        "\n",
        "  #multi head self attention\n",
        "  self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "\n",
        "  #feed forward network\n",
        "  self.ff = nn.Sequential(\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(d_model * 4, d_model),\n",
        "      nn.Dropout(0.2)\n",
        "  )\n",
        "\n",
        "  #layer normalization + residuals\n",
        "  self.norm1 = nn.LayerNorm(d_model)\n",
        "  self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    attn_out, _ = self.attn(x, x, x) #apply MHSA --> x is passed three times for all QKV values --> outputs attention weights\n",
        "    x = self.norm1(x + attn_out) # apply first normalization --> result is assigned back to input x\n",
        "    ff_out = self.ff(x) # input x is run through FFN\n",
        "    return self.norm2(x + ff_out) #apply second normalization after FFN --> this is the final output of each expert\n",
        "\n",
        "#film modulation\n",
        "class MetaFilm(nn.Module):\n",
        "  def __init__(self, d_model):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(d_model, d_model * 2) #gamma and beta are each size d_model #apply linear transformations to input x to get outputs of gamma and beta parameters\n",
        "\n",
        "    def forward(self, x):\n",
        "      gamma_beta = self.linear(x)\n",
        "      return gamma_beta.chunk(2, dim = -1) #split them in half equally --> giving two tensors of the same size\n",
        "\n",
        "#gated modular layer\n",
        "class GatedModularLayer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.d_model = config.d_model\n",
        "    self.num_experts = config.num_experts\n",
        "    self.top_k = config.top_k\n",
        "\n",
        "    #create expert instances --> each is initialized with d_model and numheads in the config\n",
        "    self.experts = nn.ModuleList([Expert(config.d_model, config.num_heads) for _ in range(config.num_experts)])\n",
        "    self.meta_nets = nn.ModuleList([MetaFilm(config.d_model) for _ in range(config.num_experts)]) #create one FILM network for each expert\n",
        "    self.gate = nn.Linear(config.d_model, config.num_experts) #takes input with d_model features and outputs a tensor with num_experts features --> determines which experts are most relevant for processing a given input token (provides score output for each expert)\n",
        "    self.norm = nn.LayerNorm(config.d_model) #layer norm to normalize the combined output of the selected experts and residual connections\n",
        "\n",
        "\n",
        "    def forward(self, x, return_gate_probs=False):\n",
        "        B, T, D = x.shape\n",
        "        gate_logits = self.gate(x)  # (B, T, N) --> pass through gating network and oututs gate_logits (shape of (B, T, num_experts))\n",
        "        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=-1) #select top-k experts along the last dimension in gate_logits, store the value of the top_k largest scores + shape (vals), idx stores the indices of the experts selected\n",
        "        #this tensor stores the indices of the top_k experts with the largest scores --> its shape is also (B, T, top_k)\n",
        "\n",
        "        topk_probs = F.softmax(topk_vals, dim=-1)  # (B, T, top_k) --> the raw logits pass through softmax for probabilities --> they are the weights of each expert to the final output for that token\n",
        "\n",
        "\n",
        "        output = torch.zeros_like(x) #output tensor with same shape as the input x filled with zeroes. this will hold all the weights\n",
        "        balance = torch.zeros(self.num_experts, device=x.device) #new tensor to track the load balance across all experts --> used to calculate a load balancing loss to help gate to distribute tokens more evenly among experts\n",
        "        entropy = (-topk_probs * torch.log(topk_probs + 1e-8)).sum() #calculates the entropy of the top_k probs--> used as auxillary loss to encourage the gating probabilbites to be less peaked, leading to better exploration of experts during training (1e-8 added to prevent log of zero)\n",
        "\n",
        "\n",
        "        for i in range(self.num_experts):\n",
        "            mask = (topk_idx == i).any(-1)  # create boolean mask of shape (B, T), a value is true at (b, t) if expert i is among the top_k selected experts for each token at batch index b and sequence index i\n",
        "            if not mask.any(): #if not, the look skips to the next expert\n",
        "                continue\n",
        "\n",
        "\n",
        "            x_mask = x.clone() #copy of input x is created, and for the tokens where expert i was not selected, the input features are set to zero (to make sure the expert i only processes relevant tokens)\n",
        "            x_mask[~mask] = 0\n",
        "\n",
        "            #masked input passed through current expert network + corresponding metafilm to get gamma and beta (this one uses original token for modulation on original token features)\n",
        "            expert_out = self.experts[i](x_mask)\n",
        "            gamma, beta = self.meta_nets[i](x)\n",
        "            modulated = gamma * expert_out + beta #output of expert is modulated using the generated gamma and beta parameters from above\n",
        "\n",
        "\n",
        "            for b in range(B): #combine expert outputs\n",
        "                for t in range(T):\n",
        "                    if i in topk_idx[b, t]:\n",
        "                        idx = (topk_idx[b, t] == i).nonzero(as_tuple=False)[0].item() #get actual scores and their indices\n",
        "                        prob = topk_probs[b, t, idx]\n",
        "                        output[b, t] += prob * modulated[b, t] #gating probabilities for all selected experts are combined together (weighted by the probabilities)\n",
        "                        balance[i] += prob.item() #prob of expert i for a token is added to balance\n",
        "\n",
        "\n",
        "        output = self.norm(x + output) #input x is aded back and result is passed through norm\n",
        "        balance = balance / (B * T) #total accumulated prob for each expert in balance is divded by total tokens to get average probaiblity assigned by each expert across batch and sequence (load balancing loss)\n",
        "\n",
        "\n",
        "        if return_gate_probs:\n",
        "            full_probs = torch.zeros(B, T, self.num_experts, device=x.device) #returns final output of layer, the normalized entropy, and the nromalized balance (avg)\n",
        "            for k in range(self.top_k):\n",
        "                full_probs.scatter_(-1, topk_idx[:, :, k].unsqueeze(-1),\n",
        "                                    topk_probs[:, :, k].unsqueeze(-1), reduce='add') #if true, it gets and returns the tensor that ocntains softmax probaiblites for all experts each token (with zero for non-selected experts)\n",
        "            return output, entropy / (B * T), balance, full_probs\n",
        "\n",
        "\n",
        "        return output, entropy / (B * T), balance #returns expert output, entropy, balance\n",
        "\n",
        "\n",
        "#full transformer\n",
        "class GatedModularTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #initializes bert base uncased tonkenizer --> converts raw text to numerical input IDs\n",
        "        self.embed = nn.Embedding(config.vocab_size, config.d_model) #start the token embedding layer --> create lookp table wehre each unique token ID in the vocab is mapped to dense vector\n",
        "        self.pos_enc = Positional_encoding(config.d_model, config.max_seq_len) #pos encoding\n",
        "        self.layers = nn.ModuleList([GatedModularLayer(config) for _ in range(config.num_layers)]) #MoE layers\n",
        "        self.fc_out = nn.Linear(config.d_model, config.num_classes) #map output features from the last gated modular layer to the number of output classes\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, return_gate_probs=False):\n",
        "        x = self.embed(input_ids)\n",
        "        x = self.pos_enc(x)\n",
        "        total_entropy = 0 #create loss variables\n",
        "        balances = []\n",
        "        gate_probs = None\n",
        "\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if return_gate_probs and i == 0:\n",
        "                x, entropy, balance, gate_probs = layer(x, return_gate_probs=True) #inspect the gating of the first layer\n",
        "            else:\n",
        "                x, entropy, balance = layer(x)\n",
        "            total_entropy += entropy\n",
        "            balances.append(balance)\n",
        "\n",
        "\n",
        "        out = self.fc_out(x[:, 0]) #final output layer\n",
        "        if return_gate_probs:\n",
        "            return out, total_entropy, balances, gate_probs\n",
        "        return out, total_entropy, balances\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "    dataset = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "    #subsets --> pick the ranges here\n",
        "    train_dataset = dataset['train'].select(range(1000))\n",
        "    test_dataset = dataset['validation'].select(range(300))\n",
        "\n",
        "    def preprocess(example):\n",
        "        tokens = tokenizer(example['sentence'], truncation=True, padding='max_length', max_length=config.max_seq_len)\n",
        "        return {'input_ids': tokens['input_ids'], 'label': example['label']}\n",
        "\n",
        "    #set everything into dataloader for bert uncased\n",
        "    train_dataset = train_dataset.map(preprocess)\n",
        "    test_dataset = test_dataset.map(preprocess)\n",
        "\n",
        "    train_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
        "    test_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def preprocess(example):\n",
        "        tokens = tokenizer(example['text'], truncation=True, padding='max_length', max_length=config.max_seq_len)\n",
        "        return {'input_ids': tokens['input_ids'], 'label': example['label']}\n",
        "\n",
        "    train_dataset = train_dataset.map(preprocess)\n",
        "    test_dataset = test_dataset.map(preprocess)\n",
        "\n",
        "    train_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
        "    test_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "#train\n",
        "def train_model(model, train_loader, config, epochs=8):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss, total_acc = 0, 0\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(config.device)\n",
        "            labels = batch['label'].to(config.device)\n",
        "\n",
        "\n",
        "            logits, entropy, balance = model(input_ids)\n",
        "            loss_cls = ce_loss(logits, labels)\n",
        "            avg_balance = torch.stack(balance).mean(dim=0)\n",
        "            loss_balance = ((avg_balance - 1 / config.num_experts) ** 2).sum()\n",
        "            loss = loss_cls + config.lambda_entropy * entropy + config.lambda_balance * loss_balance\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += (logits.argmax(dim=1) == labels).float().mean().item()\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Loss={total_loss:.3f}, Acc={total_acc / len(train_loader):.3f}\")\n",
        "\n",
        "\n",
        "#evaluation + visualization\n",
        "def plot_heatmap(gate_probs, input_ids, tokenizer, max_tokens=20):\n",
        "    gate_probs = gate_probs[0].cpu().detach().numpy()\n",
        "    tokens = input_ids[0][:max_tokens].cpu().tolist()\n",
        "    token_strs = tokenizer.convert_ids_to_tokens(tokens)\n",
        "    data = gate_probs[:max_tokens].T\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(max_tokens * 0.4, 4))\n",
        "    sns.heatmap(data, annot=True, fmt=\".2f\", cmap=\"viridis\",\n",
        "                xticklabels=token_strs, yticklabels=[f'Expert {i}' for i in range(data.shape[0])])\n",
        "    plt.xlabel(\"tokens\")\n",
        "    plt.ylabel(\"experts\")\n",
        "    plt.title(\"Gate Probabilities Heatmap (Top-k Experts)\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def eval_visual(model, test_loader, config):\n",
        "    model.eval()\n",
        "    batch = next(iter(test_loader))\n",
        "    input_ids = batch['input_ids'].to(config.device)\n",
        "    labels = batch['label'].to(config.device)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, entropy, balances, gate_probs = model(input_ids, return_gate_probs=True)\n",
        "        acc = (logits.argmax(dim=1) == labels).float().mean().item()\n",
        "        print(f\"eval accuracy: {acc:.3f}\")\n",
        "\n",
        "\n",
        "    plot_heatmap(gate_probs, input_ids, model.tokenizer)\n",
        "\n",
        "\n",
        "#main\n",
        "if __name__ == \"__main__\":\n",
        "    model = GatedModularTransformer(config).to(config.device)\n",
        "    train_loader, test_loader = get_dataloaders(config)\n",
        "\n",
        "    print(\"starting training\")\n",
        "    train_model(model, train_loader, config, epochs=3)\n",
        "\n",
        "    print(\"creating visuals\")\n",
        "    eval_visual(model, test_loader, config)\n",
        "\n",
        "    print(\"done\")"
      ],
      "metadata": {
        "id": "gEZKBxfiKJwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanilla Transformer\n"
      ],
      "metadata": {
        "id": "sZL_m7DwVzP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "\n",
        "class Config:\n",
        "    vocab_size = 30522\n",
        "    d_model = 128\n",
        "    num_heads = 4\n",
        "    num_layers = 2\n",
        "    max_seq_len = 128\n",
        "    batch_size = 32\n",
        "    num_classes = 3\n",
        "    dropout = 0.4\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "config = Config()\n",
        "\n",
        "\n",
        "class pos_enc(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "class plain_trans(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        #input embedding + pos enc + ecoder layers\n",
        "        self.embed = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_enc = pos_enc(config.d_model, config.max_seq_len)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=config.d_model,\n",
        "            nhead=config.num_heads,\n",
        "            dim_feedforward=4 * config.d_model,\n",
        "            dropout=config.dropout,\n",
        "            batch_first=True,\n",
        "\n",
        "\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.num_layers)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.classifier = nn.Linear(config.d_model, config.num_classes)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embed(input_ids)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)  # avg pooling over sequence\n",
        "        x = self.dropout(x)\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def compute_metrics(logits, labels):\n",
        "    preds = logits.argmax(dim=-1)\n",
        "    correct = (preds == labels).sum().item()\n",
        "    total = labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion, config):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    n = 0\n",
        "    start_time = time.perf_counter()\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        input_ids = batch['input_ids'].to(config.device)\n",
        "        labels = batch['label'].to(config.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "        total_acc += compute_metrics(logits, labels) * input_ids.size(0)\n",
        "        n += input_ids.size(0)\n",
        "\n",
        "    epoch_time = time.perf_counter() - start_time\n",
        "    return total_loss / n, total_acc / n, epoch_time\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_epoch(model, dataloader, criterion, config):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    n = 0\n",
        "    start_time = time.perf_counter()\n",
        "    inference_times = []\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "        input_ids = batch['input_ids'].to(config.device)\n",
        "        labels = batch['label'].to(config.device)\n",
        "\n",
        "        start_infer = time.perf_counter()\n",
        "        logits = model(input_ids)\n",
        "        torch.cuda.synchronize() if config.device == 'cuda' else None  # sync for accurate timing\n",
        "        end_infer = time.perf_counter()\n",
        "        inference_times.append(end_infer - start_infer)\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "        total_acc += compute_metrics(logits, labels) * input_ids.size(0)\n",
        "        n += input_ids.size(0)\n",
        "\n",
        "    epoch_time = time.perf_counter() - start_time\n",
        "    avg_inference_latency = sum(inference_times) / len(inference_times) if inference_times else 0\n",
        "    return total_loss / n, total_acc / n, epoch_time, avg_inference_latency\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([torch.tensor(x['input_ids']) for x in batch])\n",
        "    labels = torch.tensor([x['label'] for x in batch])\n",
        "    return {'input_ids': input_ids, 'label': labels}\n",
        "\n",
        "\n",
        "def main():\n",
        "    dataset = load_dataset(\"glue\", \"mnli\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    def tokenize_fn(batch):\n",
        "        # tokenize premise and hypothesis pairs\n",
        "        return tokenizer(batch['premise'], batch['hypothesis'],\n",
        "                         padding='max_length', max_length=config.max_seq_len,\n",
        "                         truncation=True)\n",
        "\n",
        "\n",
        "    dataset = dataset.map(tokenize_fn, batched=True)\n",
        "    dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
        "\n",
        "    train_loader = DataLoader(dataset['train'], batch_size=config.batch_size,\n",
        "                              shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(dataset['validation_matched'], batch_size=config.batch_size,\n",
        "                            shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "    model = plain_trans(config).to(config.device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) #lr step\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    epochs = 20 #set epochs here\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc, train_time = train_epoch(model, train_loader, optimizer, criterion, config)\n",
        "        val_loss, val_acc, val_time, val_infer_latency = eval_epoch(model, val_loader, criterion, config)\n",
        "        scheduler.step()\n",
        "\n",
        "        # gpu memory usage --> allocated and max allocated\n",
        "        if config.device == 'cuda':\n",
        "            gpu_mem_allocated = torch.cuda.memory_allocated() / (1024 ** 2)  # MB\n",
        "            gpu_mem_reserved = torch.cuda.memory_reserved() / (1024 ** 2)  # MB\n",
        "            gpu_mem_max_allocated = torch.cuda.max_memory_allocated() / (1024 ** 2)  # MB\n",
        "            torch.cuda.reset_peak_memory_stats()  # reset for next epoch\n",
        "        else:\n",
        "            gpu_mem_allocated = gpu_mem_reserved = gpu_mem_max_allocated = 0\n",
        "\n",
        "\n",
        "        print(f\"epoch {epoch+1} | \"\n",
        "              f\"train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train Time: {train_time:.2f}s | \"\n",
        "              f\"val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Time: {val_time:.2f}s | \"\n",
        "              f\"val infer latency: {val_infer_latency*1000:.2f} ms | \"\n",
        "              f\"gpu Mem Allocated: {gpu_mem_allocated:.2f} MB | GPU Mem Reserved: {gpu_mem_reserved:.2f} MB | \"\n",
        "              f\"gpu Max Mem Allocated: {gpu_mem_max_allocated:.2f} MB\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "7UueB2JUV3dY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}