{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_gc4x_BJ8f2"
      },
      "outputs": [],
      "source": [
        "!pip install torch pandas numpy matplotlib scikit-learn seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "#config class\n",
        "class Config:\n",
        "    vocab_size = 30522\n",
        "    d_model = 128\n",
        "    num_heads = 4\n",
        "    num_experts = 4\n",
        "    top_k = 2\n",
        "    num_layers = 1\n",
        "    max_seq_len = 64\n",
        "    batch_size = 32\n",
        "    num_classes = 3  # 3 classes for MultiNLI\n",
        "    lr = 1e-4\n",
        "    lambda_entropy = 1 # 1/1 for specialized, 2/3 for balanced\n",
        "    lambda_balance = 1\n",
        "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Config()\n",
        "\n",
        "\n",
        "#positional encoding\n",
        "class Positional_encoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len = 512):\n",
        "    super().__init__()\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
        "\n",
        "    pe[:, 0::2] = torch.sin(position * div_term) #even indices\n",
        "    pe[:, 1::2] = torch.cos(position *div_term) #odd indices\n",
        "\n",
        "    self.pe = pe.unsqueeze(0) #add extra dimension (easier to broadcast when adding pos eoncoding to input tensor x)\n",
        "\n",
        "    def forward(self, x):\n",
        "      return x + self.pe[:, :x.size(1)].to(x.device) #add input tensor x + pos enc\n",
        "\n",
        "#expert submodules\n",
        "class Expert(nn.module):\n",
        "  super().__init__()\n",
        "\n",
        "  #multi head self attention\n",
        "  self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "\n",
        "  #feed forward network\n",
        "  self.ff = nn.Sequential(\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(0.2),\n",
        "      nn.Linear(d_model * 4, d_model),\n",
        "      nn.Dropout(0.2)\n",
        "  )\n",
        "\n",
        "  #layer normalization + residuals\n",
        "  self.norm1 = nn.LayerNorm(d_model)\n",
        "  self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    attn_out, _ = self.attn(x, x, x) #apply MHSA --> x is passed three times for all QKV values --> outputs attention weights\n",
        "    x = self.norm1(x + attn_out) # apply first normalization --> result is assigned back to input x\n",
        "    ff_out = self.ff(x) # input x is run through FFN\n",
        "    return self.norm2(x + ff_out) #apply second normalization after FFN --> this is the final output of each expert\n",
        "\n",
        "#film modulation\n",
        "class MetaFilm(nn.Module):\n",
        "  def __init__(self, d_model):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(d_model, d_model * 2) #gamma and beta are each size d_model #apply linear transformations to input x to get outputs of gamma and beta parameters\n",
        "\n",
        "    def forward(self, x):\n",
        "      gamma_beta = self.linear(x)\n",
        "      return gamma_beta.chunk(2, dim = -1) #split them in half equally --> giving two tensors of the same size\n",
        "\n",
        "#gated modular layer\n",
        "class GatedModularLayer(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.d_model = config.d_model\n",
        "    self.num_experts = config.num_experts\n",
        "    self.top_k = config.top_k\n",
        "\n",
        "    #create expert instances --> each is initialized with d_model and numheads in the config\n",
        "    self.experts = nn.ModuleList([Expert(config.d_model, config.num_heads) for _ in range(config.num_experts)])\n",
        "    self.meta_nets = nn.ModuleList([MetaFilm(config.d_model) for _ in range(config.num_experts)]) #create one FILM network for each expert\n",
        "    self.gate = nn.Linear(config.d_model, config.num_experts) #takes input with d_model features and outputs a tensor with num_experts features --> determines which experts are most relevant for processing a given input token (provides score output for each expert)\n",
        "    self.norm = nn.LayerNorm(config.d_model) #layer norm to normalize the combined output of the selected experts and residual connections\n",
        "\n",
        "\n",
        "    def forward(self, x, return_gate_probs=False):\n",
        "        B, T, D = x.shape\n",
        "        gate_logits = self.gate(x)  # (B, T, N) --> pass through gating network and oututs gate_logits (shape of (B, T, num_experts))\n",
        "        topk_vals, topk_idx = torch.topk(gate_logits, self.top_k, dim=-1) #select top-k experts along the last dimension in gate_logits, store the value of the top_k largest scores + shape (vals), idx stores the indices of the experts selected\n",
        "        #this tensor stores the indices of the top_k experts with the largest scores --> its shape is also (B, T, top_k)\n",
        "\n",
        "        topk_probs = F.softmax(topk_vals, dim=-1)  # (B, T, top_k) --> the raw logits pass through softmax for probabilities --> they are the weights of each expert to the final output for that token\n",
        "\n",
        "\n",
        "        output = torch.zeros_like(x) #output tensor with same shape as the input x filled with zeroes. this will hold all the weights\n",
        "        balance = torch.zeros(self.num_experts, device=x.device) #new tensor to track the load balance across all experts --> used to calculate a load balancing loss to help gate to distribute tokens more evenly among experts\n",
        "        entropy = (-topk_probs * torch.log(topk_probs + 1e-8)).sum() #calculates the entropy of the top_k probs--> used as auxillary loss to encourage the gating probabilbites to be less peaked, leading to better exploration of experts during training (1e-8 added to prevent log of zero)\n",
        "\n",
        "\n",
        "        for i in range(self.num_experts):\n",
        "            mask = (topk_idx == i).any(-1)  # create boolean mask of shape (B, T), a value is true at (b, t) if expert i is among the top_k selected experts for each token at batch index b and sequence index i\n",
        "            if not mask.any(): #if not, the look skips to the next expert\n",
        "                continue\n",
        "\n",
        "\n",
        "            x_mask = x.clone() #copy of input x is created, and for the tokens where expert i was not selected, the input features are set to zero (to make sure the expert i only processes relevant tokens)\n",
        "            x_mask[~mask] = 0\n",
        "\n",
        "            #masked input passed through current expert network + corresponding metafilm to get gamma and beta (this one uses original token for modulation on original token features)\n",
        "            expert_out = self.experts[i](x_mask)\n",
        "            gamma, beta = self.meta_nets[i](x)\n",
        "            modulated = gamma * expert_out + beta #output of expert is modulated using the generated gamma and beta parameters from above\n",
        "\n",
        "\n",
        "            for b in range(B): #combine expert outputs\n",
        "                for t in range(T):\n",
        "                    if i in topk_idx[b, t]:\n",
        "                        idx = (topk_idx[b, t] == i).nonzero(as_tuple=False)[0].item() #get actual scores and their indices\n",
        "                        prob = topk_probs[b, t, idx]\n",
        "                        output[b, t] += prob * modulated[b, t] #gating probabilities for all selected experts are combined together (weighted by the probabilities)\n",
        "                        balance[i] += prob.item() #prob of expert i for a token is added to balance\n",
        "\n",
        "\n",
        "        output = self.norm(x + output) #input x is aded back and result is passed through norm\n",
        "        balance = balance / (B * T) #total accumulated prob for each expert in balance is divded by total tokens to get average probaiblity assigned by each expert across batch and sequence (load balancing loss)\n",
        "\n",
        "\n",
        "        if return_gate_probs:\n",
        "            full_probs = torch.zeros(B, T, self.num_experts, device=x.device) #returns final output of layer, the normalized entropy, and the nromalized balance (avg)\n",
        "            for k in range(self.top_k):\n",
        "                full_probs.scatter_(-1, topk_idx[:, :, k].unsqueeze(-1),\n",
        "                                    topk_probs[:, :, k].unsqueeze(-1), reduce='add') #if true, it gets and returns the tensor that ocntains softmax probaiblites for all experts each token (with zero for non-selected experts)\n",
        "            return output, entropy / (B * T), balance, full_probs\n",
        "\n",
        "\n",
        "        return output, entropy / (B * T), balance #returns expert output, entropy, balance\n",
        "\n",
        "\n",
        "#full transformer\n",
        "class GatedModularTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') #initializes bert base uncased tonkenizer --> converts raw text to numerical input IDs\n",
        "        self.embed = nn.Embedding(config.vocab_size, config.d_model) #start the token embedding layer --> create lookp table wehre each unique token ID in the vocab is mapped to dense vector\n",
        "        self.pos_enc = Positional_encoding(config.d_model, config.max_seq_len) #pos encoding\n",
        "        self.layers = nn.ModuleList([GatedModularLayer(config) for _ in range(config.num_layers)]) #MoE layers\n",
        "        self.fc_out = nn.Linear(config.d_model, config.num_classes) #map output features from the last gated modular layer to the number of output classes\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, return_gate_probs=False):\n",
        "        x = self.embed(input_ids)\n",
        "        x = self.pos_enc(x)\n",
        "        total_entropy = 0 #create loss variables\n",
        "        balances = []\n",
        "        gate_probs = None\n",
        "\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if return_gate_probs and i == 0:\n",
        "                x, entropy, balance, gate_probs = layer(x, return_gate_probs=True) #inspect the gating of the first layer\n",
        "            else:\n",
        "                x, entropy, balance = layer(x)\n",
        "            total_entropy += entropy\n",
        "            balances.append(balance)\n",
        "\n",
        "\n",
        "        out = self.fc_out(x[:, 0]) #final output layer\n",
        "        if return_gate_probs:\n",
        "            return out, total_entropy, balances, gate_probs\n",
        "        return out, total_entropy, balances\n",
        "\n",
        "def get_dataloaders(config):\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "    dataset = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "    #subsets --> pick the ranges here\n",
        "    train_dataset = dataset['train'].select(range(1000))\n",
        "    test_dataset = dataset['validation'].select(range(300))\n",
        "\n",
        "    def preprocess(example):\n",
        "        tokens = tokenizer(example['sentence'], truncation=True, padding='max_length', max_length=config.max_seq_len)\n",
        "        return {'input_ids': tokens['input_ids'], 'label': example['label']}\n",
        "\n",
        "    #set everything into dataloader for bert uncased\n",
        "    train_dataset = train_dataset.map(preprocess)\n",
        "    test_dataset = test_dataset.map(preprocess)\n",
        "\n",
        "    train_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
        "    test_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def preprocess(example):\n",
        "        tokens = tokenizer(example['text'], truncation=True, padding='max_length', max_length=config.max_seq_len)\n",
        "        return {'input_ids': tokens['input_ids'], 'label': example['label']}\n",
        "\n",
        "    train_dataset = train_dataset.map(preprocess)\n",
        "    test_dataset = test_dataset.map(preprocess)\n",
        "\n",
        "    train_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
        "    test_dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "#train\n",
        "def train_model(model, train_loader, config, epochs=8):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss, total_acc = 0, 0\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(config.device)\n",
        "            labels = batch['label'].to(config.device)\n",
        "\n",
        "\n",
        "            logits, entropy, balance = model(input_ids)\n",
        "            loss_cls = ce_loss(logits, labels)\n",
        "            avg_balance = torch.stack(balance).mean(dim=0)\n",
        "            loss_balance = ((avg_balance - 1 / config.num_experts) ** 2).sum()\n",
        "            loss = loss_cls + config.lambda_entropy * entropy + config.lambda_balance * loss_balance\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += (logits.argmax(dim=1) == labels).float().mean().item()\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}: Loss={total_loss:.3f}, Acc={total_acc / len(train_loader):.3f}\")\n",
        "\n",
        "\n",
        "#evaluation + visualization\n",
        "def plot_heatmap(gate_probs, input_ids, tokenizer, max_tokens=20):\n",
        "    gate_probs = gate_probs[0].cpu().detach().numpy()\n",
        "    tokens = input_ids[0][:max_tokens].cpu().tolist()\n",
        "    token_strs = tokenizer.convert_ids_to_tokens(tokens)\n",
        "    data = gate_probs[:max_tokens].T\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(max_tokens * 0.4, 4))\n",
        "    sns.heatmap(data, annot=True, fmt=\".2f\", cmap=\"viridis\",\n",
        "                xticklabels=token_strs, yticklabels=[f'Expert {i}' for i in range(data.shape[0])])\n",
        "    plt.xlabel(\"tokens\")\n",
        "    plt.ylabel(\"experts\")\n",
        "    plt.title(\"Gate Probabilities Heatmap (Top-k Experts)\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def eval_visual(model, test_loader, config):\n",
        "    model.eval()\n",
        "    batch = next(iter(test_loader))\n",
        "    input_ids = batch['input_ids'].to(config.device)\n",
        "    labels = batch['label'].to(config.device)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, entropy, balances, gate_probs = model(input_ids, return_gate_probs=True)\n",
        "        acc = (logits.argmax(dim=1) == labels).float().mean().item()\n",
        "        print(f\"eval accuracy: {acc:.3f}\")\n",
        "\n",
        "\n",
        "    plot_heatmap(gate_probs, input_ids, model.tokenizer)\n",
        "\n",
        "\n",
        "#main\n",
        "if __name__ == \"__main__\":\n",
        "    model = GatedModularTransformer(config).to(config.device)\n",
        "    train_loader, test_loader = get_dataloaders(config)\n",
        "\n",
        "    print(\"starting training\")\n",
        "    train_model(model, train_loader, config, epochs=3)\n",
        "\n",
        "    print(\"creating visuals\")\n",
        "    eval_visual(model, test_loader, config)\n",
        "\n",
        "    print(\"done\")"
      ],
      "metadata": {
        "id": "gEZKBxfiKJwq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}